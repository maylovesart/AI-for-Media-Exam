{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM4eKKENAqmPAWTsSjgZN6c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maylovesart/AI-for-Media-Exam/blob/main/UK_Black_Icons_GANs_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG9h4UZDTzGS",
        "outputId": "8dabe8b8-8f71-4a14-d7d8-3fbef8dbaacd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n",
            "Loaded 12 content images and 20 style images.\n",
            "Starting training for 1000 epochs...\n",
            "Epoch 50/1000, Time: 0.46 sec\n",
            "Generator Loss: 3.3979, Discriminator Loss: 0.4020\n",
            "Epoch 100/1000, Time: 0.43 sec\n",
            "Generator Loss: 3.4230, Discriminator Loss: 0.4617\n",
            "Epoch 150/1000, Time: 0.42 sec\n",
            "Generator Loss: 4.5336, Discriminator Loss: 0.3754\n",
            "Epoch 200/1000, Time: 0.43 sec\n",
            "Generator Loss: 2.3865, Discriminator Loss: 0.5227\n",
            "Epoch 250/1000, Time: 0.44 sec\n",
            "Generator Loss: 4.0772, Discriminator Loss: 0.4585\n",
            "Epoch 300/1000, Time: 0.43 sec\n",
            "Generator Loss: 2.2390, Discriminator Loss: 0.5673\n",
            "Epoch 350/1000, Time: 0.43 sec\n",
            "Generator Loss: 2.6776, Discriminator Loss: 0.5783\n",
            "Epoch 400/1000, Time: 0.43 sec\n",
            "Generator Loss: 1.6828, Discriminator Loss: 0.6740\n",
            "Epoch 450/1000, Time: 0.43 sec\n",
            "Generator Loss: 2.1872, Discriminator Loss: 0.7476\n",
            "Epoch 500/1000, Time: 0.43 sec\n",
            "Generator Loss: 1.0608, Discriminator Loss: 0.8940\n",
            "Epoch 550/1000, Time: 0.43 sec\n",
            "Generator Loss: 1.7818, Discriminator Loss: 0.6972\n",
            "Epoch 600/1000, Time: 0.43 sec\n",
            "Generator Loss: 1.8247, Discriminator Loss: 0.7297\n",
            "Epoch 650/1000, Time: 0.43 sec\n",
            "Generator Loss: 2.1733, Discriminator Loss: 0.7143\n",
            "Epoch 700/1000, Time: 0.43 sec\n",
            "Generator Loss: 1.6132, Discriminator Loss: 0.6538\n",
            "Epoch 750/1000, Time: 0.43 sec\n",
            "Generator Loss: 2.2126, Discriminator Loss: 0.6477\n",
            "Epoch 800/1000, Time: 0.43 sec\n",
            "Generator Loss: 1.6407, Discriminator Loss: 0.6835\n",
            "Epoch 850/1000, Time: 0.44 sec\n",
            "Generator Loss: 2.0190, Discriminator Loss: 0.6078\n",
            "Epoch 900/1000, Time: 0.43 sec\n",
            "Generator Loss: 1.8293, Discriminator Loss: 0.5992\n",
            "Epoch 950/1000, Time: 0.43 sec\n",
            "Generator Loss: 2.0831, Discriminator Loss: 0.5962\n",
            "Epoch 1000/1000, Time: 0.43 sec\n",
            "Generator Loss: 1.2548, Discriminator Loss: 0.7709\n",
            "Training complete.\n",
            "\n",
            "Ethical Considerations:\n",
            "This project involves generating images based on real individuals.\n",
            "It's important to consider the ethical implications of creating and using such images.\n",
            "The generated images should not be used for misrepresentation or without proper context.\n",
            "\n",
            "LLM Disclaimer:\n",
            "Large Language Models were used for code structuring and debugging assistance.\n",
            "All implementation decisions and parameter tuning were made by the human developer.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "# Set TensorFlow to use GPU\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "# Load and Preprocess Images\n",
        "TARGET_SIZE = (256, 256)\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_image(img, channels=3)\n",
        "    img = tf.image.resize(img, TARGET_SIZE)\n",
        "    img = img / 127.5 - 1  # Normalize to [-1, 1]\n",
        "    return img\n",
        "\n",
        "def load_and_preprocess_images(directory):\n",
        "    images = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
        "            img_path = os.path.join(directory, filename)\n",
        "            img = preprocess_image(img_path)\n",
        "            images.append(img)\n",
        "    return tf.stack(images)\n",
        "\n",
        "# Load images from the specified directories\n",
        "content_images = load_and_preprocess_images(\"/content/Diane Abbott\")\n",
        "style_images = load_and_preprocess_images(\"/content/Richard Stone\")\n",
        "\n",
        "print(f\"Loaded {len(content_images)} content images and {len(style_images)} style images.\")\n",
        "\n",
        "# Data Augmentation\n",
        "def augment_image(image):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
        "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
        "    return image\n",
        "\n",
        "# Spectral Normalization\n",
        "class SpectralNormalization(tf.keras.constraints.Constraint):\n",
        "    def __init__(self):\n",
        "        self.iteration = 0\n",
        "\n",
        "    def __call__(self, w):\n",
        "        self.iteration += 1\n",
        "        if self.iteration % 50 == 0:\n",
        "            return tf.nn.l2_normalize(w, axis=None)\n",
        "        return w\n",
        "\n",
        "# GAN Model Definition\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(16 * 16 * 256, use_bias=False, input_shape=(200,)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "        tf.keras.layers.Reshape((16, 16, 256)),\n",
        "        tf.keras.layers.Conv2DTranspose(128, 5, strides=2, padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "        tf.keras.layers.Conv2DTranspose(64, 5, strides=2, padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "        tf.keras.layers.Conv2DTranspose(32, 5, strides=2, padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "        tf.keras.layers.Conv2DTranspose(3, 5, strides=2, padding='same', activation='tanh')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(64, 5, strides=2, padding='same', input_shape=[256, 256, 3]),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Conv2D(128, 5, strides=2, padding='same'),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Conv2D(256, 5, strides=2, padding='same'),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Loss Functions\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output) * 0.9, real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "# Learning Rate Schedule\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-4,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.96\n",
        ")\n",
        "\n",
        "# Optimizers\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.5)\n",
        "\n",
        "# Training Step\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    batch_size = tf.shape(images)[0]\n",
        "    noise = tf.random.normal([batch_size, 200])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n",
        "\n",
        "# Generate and Save Images\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "    predictions = model(test_input, training=False)\n",
        "\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(4, 4, i+1)\n",
        "        plt.imshow(predictions[i, :, :, :] * 0.5 + 0.5)  # Denormalize\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.savefig(f'/content/generated_image_epoch_{epoch}.png')\n",
        "    plt.close()\n",
        "\n",
        "# Training Loop\n",
        "def train(dataset, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "\n",
        "        for image_batch in dataset:\n",
        "            gen_loss, disc_loss = train_step(image_batch)\n",
        "\n",
        "        # Print updates every 50 epochs\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Time: {time.time()-start:.2f} sec\")\n",
        "            print(f\"Generator Loss: {gen_loss:.4f}, Discriminator Loss: {disc_loss:.4f}\")\n",
        "\n",
        "        # Generate and save images every 100 epochs\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            generate_and_save_images(generator, epoch + 1, seed)\n",
        "\n",
        "    # Generate a final set of images\n",
        "    generate_and_save_images(generator, epochs, seed)\n",
        "\n",
        "# Prepare the dataset\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 1000\n",
        "\n",
        "# Combine content and style images\n",
        "combined_images = tf.concat([content_images, style_images], axis=0)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(combined_images)\n",
        "dataset = dataset.map(augment_image)  # Apply data augmentation\n",
        "dataset = dataset.shuffle(1000).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Initialize models\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "# Apply spectral normalization to the discriminator\n",
        "for layer in discriminator.layers:\n",
        "    if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
        "        layer.kernel_constraint = SpectralNormalization()\n",
        "\n",
        "# Create a seed for image generation\n",
        "seed = tf.random.normal([16, 200])\n",
        "\n",
        "# Start training\n",
        "print(\"Starting training for 1000 epochs...\")\n",
        "train(dataset, EPOCHS)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Ethical considerations and LLM disclaimer\n",
        "print(\"\\nEthical Considerations:\")\n",
        "print(\"This project involves generating images based on real individuals.\")\n",
        "print(\"It's important to consider the ethical implications of creating and using such images.\")\n",
        "print(\"The generated images should not be used for misrepresentation or without proper context.\")\n",
        "\n",
        "print(\"\\nLLM Disclaimer:\")\n",
        "print(\"Large Language Models were used for code structuring and debugging assistance.\")\n",
        "print(\"All implementation decisions and parameter tuning were made by the human developer.\")"
      ]
    }
  ]
}